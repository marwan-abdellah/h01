{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad42837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neurons: 15567\n",
      "Pyramidals: 8626\n",
      "Pyramidals: L1 555\n",
      "Pyramidals: L2 4573\n",
      "Pyramidals: L3 2639\n",
      "Pyramidals: L4 3388\n",
      "Pyramidals: L5 2268\n",
      "Pyramidals: L6 1045\n",
      "Interneurons: 687\n",
      "Interneurons: L1 77\n",
      "Interneurons: L2 253\n",
      "Interneurons: L3 137\n",
      "Interneurons: L4 94\n",
      "Interneurons: L5 88\n",
      "Interneurons: L6 25\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def save_ids_to_txt_file(df, file_path):\n",
    "    \"\"\"\n",
    "    Save the IDs from the DataFrame to a text file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        for index, row in df.iterrows():\n",
    "            f.write(f\"{row['id']}\\n\")\n",
    "    \n",
    "# Load the CSV file\n",
    "df = pd.read_csv('data/h01.csv')\n",
    "\n",
    "# Filter the neurons based on the 'data' column\n",
    "df_neurons = df[df['data'].str.contains('#neuron')]\n",
    "print(\"Neurons:\", len(df_neurons))\n",
    "save_ids_to_txt_file(df_neurons, 'neurons.ids')\n",
    "\n",
    "# Filter the pyramidal cells based on the 'data' column\n",
    "df_pyramidal = df_neurons[df_neurons['data'].str.contains('#pyramidal ')]\n",
    "print(\"Pyramidals:\", len(df_pyramidal))\n",
    "save_ids_to_txt_file(df_pyramidal, 'pyramidal.ids')\n",
    "\n",
    "# Get the puyramidal neurons in all layers \n",
    "for i in [1, 2, 3, 4, 5, 6]:\n",
    "    df_pyramidal_layer = df_neurons[df_neurons['data'].str.contains(f'#L{i}')]\n",
    "    print(f\"Pyramidals: L{i}\", len(df_pyramidal_layer))\n",
    "    save_ids_to_txt_file(df_pyramidal_layer, f'pyramidal_L{i}.ids')\n",
    "    \n",
    "\n",
    "# Filter the interneuron cells based on the 'data' column\n",
    "df_interneuron = df_neurons[df_neurons['data'].str.contains('#interneuron ')]\n",
    "print(\"Interneurons:\", len(df_interneuron))\n",
    "save_ids_to_txt_file(df_interneuron, 'interneurons.ids')\n",
    "\n",
    "for i in [1, 2, 3, 4, 5, 6]:\n",
    "    df_interneuron_layer = df_interneuron[df_interneuron['data'].str.contains(f'#L{i}')]\n",
    "    print(f\"Interneurons: L{i}\", len(df_interneuron_layer))\n",
    "    save_ids_to_txt_file(df_interneuron_layer, f'interneurons_L{i}.ids')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5784bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Tokens need to be acquired by hand. Please follow the following steps:\n",
      "                1) Go to: https://global.brain-wire-test.org//auth/api/v1/create_token to create a new token.\n",
      "                2) Log in with your Google credentials and copy the token shown afterward.\n",
      "                3a) Save it to your computer with: client.auth.save_token(token=\"PASTE_YOUR_TOKEN_HERE\")\n",
      "                or\n",
      "                3b) Set it for the current session only with client.auth.token = \"PASTE_YOUR_TOKEN_HERE\"\n",
      "                Note: If you need to save or load multiple tokens, please read the documentation for details.\n",
      "                Warning! Creating a new token by finishing step 2 will invalidate the previous token!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening in existing browser session.\n"
     ]
    }
   ],
   "source": [
    "import caveclient\n",
    "url = \"https://global.brain-wire-test.org/\"\n",
    "auth = caveclient.auth.AuthClient(server_address=url)\n",
    "auth.setup_token(make_new=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6cf43d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_token = \"be1bde27490bb4f37580cc370ba9068a\"\n",
    "auth.save_token(token=auth_token, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7514f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is already there!\n",
      "8841fffdac923d4cee7b07fe82e11987\n"
     ]
    }
   ],
   "source": [
    "from caveclient import CAVEclient\n",
    "client = CAVEclient()\n",
    "try:\n",
    "    client.auth.save_token(token=auth_token)\n",
    "except :\n",
    "    print(\"Token is already there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389d558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cloudvolume\n",
    "\n",
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def get_cave_client(dataset):\n",
    "    \"\"\"\n",
    "    get_cave_client Getting the CAVE client\n",
    "\n",
    "    Args:\n",
    "        dataset (_type_): Examples ['minnie65_public_v117', 'h01-release'] \n",
    "    \"\"\"\n",
    "    # Create the CAVE client\n",
    "    client = caveclient.CAVEclient(dataset, auth_token=auth_token)\n",
    "    \n",
    "    # To access dynamic meshes, you can query the segmentation source from the info client\n",
    "    # client.info.segmentation_source()\n",
    "    print(client.info.get_datastack_info())\n",
    "    \n",
    "    # Return the client\n",
    "    return client\n",
    "\n",
    "\n",
    "def get_cloud_volume(path):\n",
    "    \"\"\"\n",
    "    get_cloud_volume Getting the cloud volume\n",
    "\n",
    "    Returns:\n",
    "        _type_: Example: \"precomputed://gs://iarpa_microns/minnie/minnie65/seg\"\n",
    "    \"\"\"    \n",
    "    # Return the cloud volume \n",
    "    print(\"Getting the cloud volume\")\n",
    "    return cloudvolume.CloudVolume(path, use_https=True, progress=True)\n",
    "\n",
    "\n",
    "def load_ids_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        ids = list()\n",
    "        for l in f:\n",
    "            ids.append(l.strip('\\n'))\n",
    "    f.close()\n",
    "    return ids\n",
    "\n",
    "####################################################################################################\n",
    "# @download_cell_from_cloud_volume\n",
    "####################################################################################################\n",
    "def download_cell_from_cloud_volume(cloud_volume, cell_id, output_directory):    \n",
    "\n",
    "    try:\n",
    "        # Load the mesh \n",
    "        mesh = cloud_volume.mesh.get(cell_id)[cell_id]\n",
    "\n",
    "        # Write the mesh  \n",
    "        # TODO: Add an option to save it as .h5 file \n",
    "        f = open('%s/%s.obj' % (output_directory, str(cell_id)), 'wb')\n",
    "        f.write(mesh.to_obj())\n",
    "        f.close()\n",
    "    except Exception as e:\n",
    "        print(\"Error in downloading the cell %s\" % str(cell_id))\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "# @download_cells_from_cloud_volume\n",
    "####################################################################################################  \n",
    "def download_cells_from_cloud_volume(cloud_volume, cell_ids, output_directory):\n",
    "    \"\"\"\n",
    "    download_cells_from_cloud_volume Downloading the cells from the cloud volume\n",
    "\n",
    "    Args:\n",
    "        cloud_volume (_type_): _description_\n",
    "        cell_ids (_type_): _description_\n",
    "        output_directory (_type_): _description_\n",
    "    \"\"\"    \n",
    "    # Loop through the cell ids\n",
    "    for cell_id in cell_ids:\n",
    "        if (int(cell_id) > 0):\n",
    "            print('Loading [%s]' % cell_id)\n",
    "            download_cell_from_cloud_volume(cloud_volume, int(cell_id), output_directory)\n",
    "        \n",
    "        \n",
    "def download_cells_from_cloud_volume_parallel(cloud_volume, cell_ids, output_directory, n_cores=8):\n",
    "    \"\"\"\n",
    "    download_cells_from_cloud_volume Downloading the cells from the cloud volume\n",
    "\n",
    "    Args:\n",
    "        cloud_volume (_type_): _description_\n",
    "        cell_ids (_type_): _description_\n",
    "        output_directory (_type_): _description_\n",
    "    \"\"\"    \n",
    "    \n",
    "    commands = list()\n",
    "    for i in cell_ids:\n",
    "        commands.append([int(i), output_directory])\n",
    "        \n",
    "    from joblib import Parallel, delayed\n",
    "    import multiprocessing\n",
    "    Parallel(n_jobs=int(n_cores))(delayed(download_cell_from_cloud_volume)(\n",
    "        cloud_volume, i[0], i[1]) for i in commands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e6545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_file = \"/scratch/h01-data/ids/pyramidal_L5.ids\"\n",
    "output_directory = \"/scratch/h01-data/meshes/pyramidal_L5\"\n",
    "\n",
    "ids = load_ids_from_file(ids_file)\n",
    "print(\"Number of ids: \", len(ids))\n",
    "\n",
    "# Create the output directory\n",
    "create_dir(output_directory)\n",
    "\n",
    "# Craete the cloud volume\n",
    "cloud_volume_path = \"precomputed://gs://h01-release/data/20210601/c3\"\n",
    "cloud_volume = get_cloud_volume(cloud_volume_path)\n",
    "\n",
    "# Download the cells from the cloud volume\n",
    "download_cells_from_cloud_volume(cloud_volume, ids, output_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
